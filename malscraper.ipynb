{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Interface with SQL\n",
    "def run_query(DB, q):\n",
    "    with sqlite3.connect(DB) as conn:\n",
    "        return pd.read_sql(q,conn)\n",
    "\n",
    "def run_command(DB, c):\n",
    "    with sqlite3.connect(DB) as conn:\n",
    "        conn.execute('PRAGMA foreign_keys = ON;')\n",
    "        conn.isolation_level = None\n",
    "        conn.execute(c)\n",
    "        \n",
    "def run_inserts(DB, c, values):\n",
    "    with sqlite3.connect(DB) as conn:\n",
    "        conn.execute('PRAGMA foreign_keys = ON;')\n",
    "        conn.isolation_level = None\n",
    "        conn.execute(c, values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Studios scraper, only one page\n",
    "def scrape_studios(DB='anime.db'):\n",
    "\n",
    "    start_time = time.time()\n",
    "    insert_query = '''\n",
    "    INSERT OR IGNORE INTO studios(\n",
    "        studio_id,  \n",
    "        studio_name\n",
    "        ) \n",
    "    VALUES (?, ?)\n",
    "    '''\n",
    "\n",
    "    #Create a special entry for unknown studios\n",
    "    insert_special = '''\n",
    "    INSERT OR IGNORE INTO studios(\n",
    "        studio_id,\n",
    "        studio_name\n",
    "        )\n",
    "    VALUES (9999, 'Unknown')\n",
    "    '''\n",
    "\n",
    "    run_command(DB, insert_special)\n",
    "\n",
    "    #Makes the request\n",
    "    url = 'https://myanimelist.net/anime/producer'\n",
    "    headers = {\n",
    "        \"User-Agent\": \"mal review scraper for research.\"\n",
    "    }\n",
    "\n",
    "    #Handle timeouts\n",
    "    try:\n",
    "        response = get(url, headers=headers, timeout = 10)\n",
    "    except:\n",
    "        print('Request timeout')\n",
    "\n",
    "    #Dump failed queries into a list        \n",
    "    failed_queries = []\n",
    "\n",
    "    #Creates the soup object    \n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    total_studios = len(html_soup.find_all('a', class_ = 'genre-name-link'))\n",
    "    for i in range(total_studios):\n",
    "        result = html_soup.find_all('a', class_ = 'genre-name-link')[i].attrs['href'].replace('/anime/producer/', '').split('/', 1)\n",
    "        studio_id = result[0]\n",
    "        studio_name = result[1]\n",
    "\n",
    "        #Write into SQL database\n",
    "        try:\n",
    "            run_inserts(DB, insert_query,(\n",
    "                int(studio_id), studio_name)\n",
    "            )\n",
    "        except:\n",
    "            print('Insert Failed {}'.format(studio_name))\n",
    "            failed_queries.append(studio_name)\n",
    "            pass\n",
    "\n",
    "        #Provide stats for monitoring\n",
    "        print('Scraping: {}'.format(url))\n",
    "        print('Inserted into database: \\'{}\\''.format(studio_name)) \n",
    "        clear_output(wait = True)\n",
    "\n",
    "    print('Scrape Complete')\n",
    "    print('Processing time: {} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tags scraper, only one page\n",
    "def scrape_tags(DB='anime.db'):\n",
    "    DB = 'anime.db'\n",
    "    start_time = time.time()\n",
    "\n",
    "    insert_query = '''\n",
    "    INSERT OR IGNORE INTO tags(\n",
    "        tag_id,  \n",
    "        tag_name\n",
    "        ) \n",
    "    VALUES (?, ?)\n",
    "    '''\n",
    "\n",
    "    #Makes the request\n",
    "    url = 'https://myanimelist.net/anime.php'\n",
    "    headers = {\n",
    "        \"User-Agent\": \"mal review scraper for research.\"\n",
    "    }\n",
    "\n",
    "    #Handle timeouts\n",
    "    try:\n",
    "        response = get(url, headers=headers, timeout = 10)\n",
    "    except:\n",
    "        print('Request timeout')\n",
    "\n",
    "    #Dump failed queries into a list\n",
    "    failed_queries = []\n",
    "\n",
    "    #Create the soup object\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    total_tags = len(html_soup.find_all('div', class_ = 'genre-link')[0].find_all('a', class_='genre-name-link'))   \n",
    "    for i in range(total_tags):\n",
    "        result = html_soup.find_all('a', class_='genre-name-link')[i].attrs['href'].replace('/anime/genre/', '').split('/', 1)\n",
    "        tag_id = result[0]\n",
    "        tag_name = result[1]\n",
    "        #Write into SQL database\n",
    "        try:\n",
    "            run_inserts(DB, insert_query,(\n",
    "                int(tag_id), tag_name)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Failed to insert into animes for tag_id: {0}, {1}'.format(tag_id, e))\n",
    "            failed_queries.append(tag_id)\n",
    "            pass\n",
    "\n",
    "        #Provide stats for monitoring\n",
    "        print('Scraping: {}'.format(url))\n",
    "        print('Inserted into database: \\'{}\\''.format(tag_name)) \n",
    "        clear_output(wait = True)\n",
    "\n",
    "    print('Scrape Complete')\n",
    "    print('Processing time: {} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Animes, anime_tags scraper\n",
    "def scrape_animes(DB='anime.db', sleep_min=9, sleep_max=18):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    insert_query1 = '''\n",
    "    INSERT OR IGNORE INTO animes(\n",
    "        anime_id,\n",
    "        studio_id,\n",
    "        anime_name,        \n",
    "        episodes_total,\n",
    "        source_material,\n",
    "        air_date,\n",
    "        overall_rating,\n",
    "        members,\n",
    "        synopsis\n",
    "        )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    '''\n",
    "\n",
    "    insert_query2 = '''\n",
    "    INSERT OR IGNORE INTO anime_tags(\n",
    "        anime_id,\n",
    "        tag_id\n",
    "        ) \n",
    "    VALUES (?, ?)\n",
    "    '''\n",
    "\n",
    "    #Makes the initial request, only once\n",
    "    url = 'https://myanimelist.net/anime.php'\n",
    "    headers = {\n",
    "        \"User-Agent\": \"mal review scraper for research.\"\n",
    "    }\n",
    "    try:\n",
    "        response = get(url, headers=headers, timeout = 10)\n",
    "    except:\n",
    "        print('Request timeout')\n",
    "\n",
    "    #Create the soup object to calculate the number of tags\n",
    "    html_soup_initial = BeautifulSoup(response.text, 'html.parser')\n",
    "    total_tags = len(html_soup_initial.find_all('div', class_ = 'genre-link')[0].find_all('a', class_='genre-name-link')) \n",
    "\n",
    "\n",
    "    requests = 0\n",
    "    #Start loop for each tag\n",
    "    for j in range(total_tags):\n",
    "        tag_details = html_soup_initial.find_all('a', class_='genre-name-link')[j]\n",
    "        total_animes = int(tag_details.text.split('(')[1].replace(')', '').replace(',', ''))\n",
    "        link_value = tag_details.attrs['href'].replace('/anime/genre/', '').split('/')[0]\n",
    "\n",
    "        #Start loop for each page within the tag\n",
    "        for i in range(math.ceil(total_animes/100)):\n",
    "\n",
    "            url = 'https://myanimelist.net/anime/genre/{0}/?page={1}'.format(link_value, i+1)\n",
    "            headers = {\n",
    "                \"User-Agent\": \"mal review scraper for research.\"\n",
    "            }\n",
    "            print('Scraping: {}'.format(url))\n",
    "\n",
    "\n",
    "            #Handle timeouts\n",
    "            try:\n",
    "                response = get(url, headers=headers, timeout = 10)\n",
    "            except:\n",
    "                print('Request timeout')\n",
    "                pass\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "                pass\n",
    "\n",
    "            #Creates the soup object    \n",
    "            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            containers = html_soup.find_all('div', class_='seasonal-anime')\n",
    "            for container in containers:\n",
    "\n",
    "                #Primary key for 'animes'\n",
    "                anime_id = container.find('div', class_='genres js-genre').attrs['id']\n",
    "\n",
    "                #Foreign key for 'animes', use 9999 for unknown studios\n",
    "                try:\n",
    "                    studio_id = container.find('span', class_='producer').find('a').attrs['href'].replace('/anime/producer/', '').split('/')[0]\n",
    "                except:\n",
    "                    studio_id = 9999\n",
    "\n",
    "                #Anime info\n",
    "                anime_name = container.find('a', class_='link-title').text            \n",
    "                episodes_total = container.find('div', class_='eps').text.strip().split(' ')[0]\n",
    "                source_material = container.find('span', class_='source').text\n",
    "                air_date = container.find('span', class_='remain-time').text.strip()\n",
    "                members = container.find('span', class_='member').text.strip().replace(',', '')\n",
    "                synopsis = container.find('span', class_='preline').text.strip().replace('\\n', '').replace('\\r', '')\n",
    "                try:\n",
    "                    overall_rating = float(container.find('span', class_='score').text.strip())\n",
    "                except:\n",
    "                    overall_rating = 'null'\n",
    "\n",
    "                #Write into SQL database, table: animes\n",
    "                try:\n",
    "                    run_inserts(DB,\n",
    "                        insert_query1,(\n",
    "                            int(anime_id), int(studio_id), anime_name, episodes_total, source_material, \\\n",
    "                            air_date, overall_rating, \\\n",
    "                            int(members), synopsis \n",
    "                        )\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print('Failed to insert into animes for anime_id: {0}, {1}'.format(anime_id, e))\n",
    "                    pass\n",
    "\n",
    "                #Container for anime_tags\n",
    "                anime_tags = container.find('div', class_=\"genres-inner\").find_all('a')\n",
    "\n",
    "                #Write into SQL database, table: animes\n",
    "                for tag in anime_tags:\n",
    "                    tag_id = tag.attrs['href'].replace('/anime/genre/', '').split('/')[0]\n",
    "                    try:\n",
    "                        run_inserts(DB,\n",
    "                            insert_query2,(\n",
    "                                int(anime_id), int(tag_id)\n",
    "                            )\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print('Failed to insert into anime_tags for anime_id: {0}, {1}'.format(anime_id, e))\n",
    "                        pass\n",
    "\n",
    "            #Provide stats for monitoring\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            requests += 1    \n",
    "\n",
    "            print('Requests Completed: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "            print('Elapased Time: {} minutes'.format(elapsed_time/60))\n",
    "            print('Pausing...')    \n",
    "            time.sleep(random.uniform(sleep_min, sleep_max))   \n",
    "            clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reviews scraper\n",
    "def scrape_reviews(DB='anime.db', page_start=1, page_end=2350, sleep_min=9, sleep_max=18):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    insert_query = '''\n",
    "    INSERT OR IGNORE INTO reviews(\n",
    "        review_id,\n",
    "        anime_id, \n",
    "        username, \n",
    "        review_date,\n",
    "        episodes_seen,\n",
    "        overall_rating,\n",
    "        story_rating,\n",
    "        animation_rating,\n",
    "        sound_rating,\n",
    "        character_rating,\n",
    "        enjoyment_rating,\n",
    "        helpful_counts,    \n",
    "        review_body\n",
    "        ) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    '''\n",
    "\n",
    "    for j in range(page_start, (page_end + 1)):\n",
    "\n",
    "        #Makes the request\n",
    "        url = 'https://myanimelist.net/reviews.php?t=anime&p={}'.format(j)\n",
    "        headers = {\n",
    "            \"User-Agent\": \"mal review scraper for research.\"\n",
    "        }\n",
    "        print('Scraping: {}'.format(url))\n",
    "\n",
    "\n",
    "        #Handle timeouts\n",
    "        try:\n",
    "            response = get(url, headers=headers, timeout = 10)\n",
    "        except:\n",
    "            print('Request timeout')\n",
    "            pass\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "            pass\n",
    "\n",
    "        #Creates the soup object    \n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_containers = html_soup.find_all('div', class_ = 'borderDark')\n",
    "\n",
    "        #Loops through the containers on a page\n",
    "        for container in review_containers:\n",
    "            review_element = container.div\n",
    "\n",
    "            #Review Id (Primary Key)\n",
    "            review_id = container.find_all(\n",
    "                'div', attrs={'style':\"float: left; display: none; margin: 0 10px 10px 0\"})[0].attrs['id'].replace('score', '')\n",
    "\n",
    "            #Anime Id (Foreign Key)\n",
    "            anime_id = review_element.find('a', class_='hoverinfo_trigger').attrs['rel'][0].replace('#revInfo', '')\n",
    "\n",
    "            #Review info\n",
    "            anime_name = (review_element.find('a', class_='hoverinfo_trigger').text)         \n",
    "            username = (review_element.find_all('td')[1].a.text)\n",
    "            review_date = (review_element.div.div.text)\n",
    "            episodes_seen = (review_element.div.find_all('div')[1].text.strip().split(' ')[0])\n",
    "            episodes_total = (review_element.div.find_all('div')[1].text.strip().split(' ')[2])\n",
    "\n",
    "            #Review ratings\n",
    "            overall_rating = (review_element.div.find_all('div')[2].text.strip().split('\\n')[1])\n",
    "            story_rating = (container.find_all('td', class_='borderClass')[3].text)\n",
    "            animation_rating = (container.find_all('td', class_='borderClass')[5].text)\n",
    "            sound_rating = (container.find_all('td', class_='borderClass')[7].text)\n",
    "            character_rating = (container.find_all('td', class_='borderClass')[9].text)       \n",
    "            enjoyment_rating = (container.find_all('td', class_='borderClass')[11].text)\n",
    "\n",
    "            #Review helpful counts\n",
    "            helpful_counts = (review_element.find('span').text)\n",
    "\n",
    "            #Review Body\n",
    "            body1 = container.select('div.spaceit.textReadability.word-break.pt8')[0].contents[4].strip()\n",
    "            body2 = container.select('div.spaceit.textReadability.word-break.pt8')[0].contents[5].text.strip()\n",
    "            review_body = (body1 + ' ' + body2).replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "            #Write into SQL database\n",
    "            try:\n",
    "                run_inserts(DB, insert_query,(\n",
    "                    int(review_id), int(anime_id), username, review_date, \\\n",
    "                    episodes_seen, int(overall_rating), \\\n",
    "                    int(story_rating), int(animation_rating), int(sound_rating), \\\n",
    "                    int(character_rating), int(enjoyment_rating), int(helpful_counts), review_body)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('Failed to scrape anime_id: {0}, {1}'.format(anime_id, e))\n",
    "                pass\n",
    "\n",
    "        #Provide stats for monitoring\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        requests = j + 1 - page_start    \n",
    "\n",
    "        print('Requests Completed: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        print('Elapased Time: {} minutes'.format(elapsed_time/60))\n",
    "        if requests == page_end - page_start + 1:\n",
    "            print('Scrape Complete')\n",
    "            break\n",
    "        print('Pausing...')    \n",
    "        time.sleep(random.uniform(sleep_min, sleep_max))   \n",
    "        clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    scrape_studios()\n",
    "    scrape_tags()\n",
    "    scrape_animes()\n",
    "    scrape_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_studios(DB='anime.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_tags(DB='anime.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_animes(DB='anime.db', sleep_min=9, sleep_max=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_reviews(DB='anime.db', page_start=1, page_end=2, sleep_min=9, sleep_max=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
